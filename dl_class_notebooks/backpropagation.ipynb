{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A(Input)\n",
    "    F(Output)\n",
    "    \n",
    "    A --> B\n",
    "    B --> C\n",
    "    C --> D\n",
    "    D --> E\n",
    "    E --> F\n",
    "\n",
    "    \n",
    "    subgraph Hidden_Layer\n",
    "        B((Weighted Sum))\n",
    "        C((Sigmoid))\n",
    "    end\n",
    "    \n",
    "    subgraph Output_Layer\n",
    "        D((Weighted Sum))\n",
    "        E((Sigmoid))\n",
    "    end\n",
    "    \n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple neural network with two layers, each one with one neuron. We'll use the sigmoid activation function for both neurons. Here's an example of backpropagation with this network:\n",
    "\n",
    "1. **Initialize the Network Parameters**:\n",
    "   Let's assume we have:\n",
    "   - Input data: $x = 0.5$\n",
    "   - Target output: $y_{\\text{target}} = 1$\n",
    "   - Random initial weights and biases for the hidden layer:\n",
    "     - $w_1 = 0.4$\n",
    "     - $b_1 = 0.5$\n",
    "   - Random initial weights and bias for the output layer:\n",
    "     - $w_2 = 0.8$\n",
    "     - $b_2 = -0.2$\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Compute the weighted sum and activation for the hidden layer:\n",
    "   \n",
    "      $z_1 = w_1 \\cdot x + b_1 = 0.4 \\cdot 0.5 + 0.5 = 0.7$\n",
    "     \n",
    "     $a_1 = \\text{sigmoid}(z_1) = \\frac{1}{1 + e^{-z_1}} = \\frac{1}{1 + e^{-0.7}} \\approx 0.668$\n",
    "\n",
    "   - Compute the weighted sum and activation for the output layer:\n",
    "\n",
    "     $z_2 = w_2 \\cdot a_1 + b_2 = 0.8 \\cdot 0.668 - 0.2 = 0.333$\n",
    "     \n",
    "     $a_2 = \\text{sigmoid}(z_2) = \\frac{1}{1 + e^{-z_2}} = \\frac{1}{1 + e^{-0.333}} \\approx 0.583$\n",
    "\n",
    "3. **Compute Loss**:\n",
    "   - Compute the loss using a simple squared error loss function:\n",
    "     \n",
    "     $L = \\frac{1}{2}(y_{\\text{target}} - a_2)^2 = \\frac{1}{2}(1 - 0.583)^2 \\approx 0.083$\n",
    "\n",
    "4. **Backpropagation**:\n",
    "   - Compute the gradient of the loss with respect to the output layer activation:\n",
    "\n",
    "     $\\frac{\\partial L}{\\partial a_2} = -(y_{\\text{target}} - a_2) = -(1 - 0.583) = -0.417$\n",
    "\n",
    "   - Compute the gradient of the output layer activation with respect to its weighted sum:\n",
    "\n",
    "     $\\frac{\\partial a_2}{\\partial z_2} = a_2 \\cdot (1 - a_2) = 0.583 \\cdot (1 - 0.583) \\approx 0.244$\n",
    "   \n",
    "   - Compute the gradient of the loss with respect to the output layer weights and bias:\n",
    "\n",
    "     $\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot a_1 = (-0.417) \\cdot 0.244 \\cdot 0.668 \\approx -0.067$\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} = (-0.417) \\cdot 0.244 \\approx -0.102$\n",
    "\n",
    "   - Compute the gradient of the loss with respect to the hidden layer activation:\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot w_2 = (-0.417) \\cdot 0.244 \\cdot 0.8 \\approx -0.080$\n",
    "\n",
    "   - Compute the gradient of the hidden layer activation with respect to its weighted sum:\n",
    "     \n",
    "     $\\frac{\\partial a_1}{\\partial z_1} = a_1 \\cdot (1 - a_1) = 0.668 \\cdot (1 - 0.668) \\approx 0.221$\n",
    "     \n",
    "   - Compute the gradient of the loss with respect to the hidden layer weights and bias:\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot x = (-0.080) \\cdot 0.221 \\cdot 0.5 \\approx -0.009$\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} = (-0.080) \\cdot 0.221 \\approx -0.018$\n",
    "\n",
    "5. **Update Weights and Bias**:\n",
    "\n",
    "   - Update the weights and bias using gradient descent (using $\\alpha = 0.1$ as the learning rate):\n",
    "\n",
    "   $w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1} = 0.4 - 0.1 \\cdot (-0.009) \\approx 0.401$\n",
    "   \n",
    "   $b_1 \\leftarrow b_1 - \\alpha \\cdot \\frac{\\partial L}{\\partial b_1} = 0.5 - 0.1 \\cdot (-0.018) \\approx 0.502$\n",
    "   \n",
    "   $w_2 \\leftarrow w_2 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2} = 0.8 - 0.1 \\cdot (-0.067) \\approx 0.807$\n",
    "   \n",
    "   $b_2 \\leftarrow b_2 - \\alpha \\cdot \\frac{\\partial L}{\\partial b_2} = -0.2 - 0.1 \\cdot (-0.102) \\approx -0.189$\n",
    "\n",
    "6. **Repeat**:\n",
    "\n",
    "   - Repeat steps 2-5 for a number of iterations or until convergence. \n",
    "   \n",
    "This example demonstrates a single iteration of forward pass, backpropagation, and weight update. In practice, you would typically perform multiple iterations of this process to train the network on a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A((Input Layer: x=2))\n",
    "    B((Neuron))\n",
    "    \n",
    "    A --> B\n",
    "    \n",
    "    subgraph Neuron\n",
    "        B\n",
    "        B --> C((Weighted Sum))\n",
    "        B --> D((Activation))\n",
    "    end\n",
    "    \n",
    "    C --> D\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course! Let's consider a simple neural network with a single neuron, one weight, and one bias. We'll perform backpropagation to update the weight and bias using the chain rule.\n",
    "\n",
    "Here's a step-by-step example:\n",
    "\n",
    "1. **Initialize the Network Parameters**:\n",
    "   Let's assume we have:\n",
    "   - Input data: \\(x = 2\\)\n",
    "   - Target output: \\(y_{\\text{target}} = 1\\)\n",
    "   - Learning rate: \\(\\alpha = 0.1\\)\n",
    "   - Random initial weight: \\(w = 0.5\\)\n",
    "   - Random initial bias: \\(b = 0.3\\)\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Compute the weighted sum:\n",
    "     \\[\n",
    "     z = w \\cdot x + b = 0.5 \\cdot 2 + 0.3 = 1.3\n",
    "     \\]\n",
    "   - Compute the activation using a linear function (since we have only one neuron, there is no activation function):\n",
    "     \\[\n",
    "     a = z = 1.3\n",
    "     \\]\n",
    "   - Compute the loss using a simple squared error loss function:\n",
    "     \\[\n",
    "     L = \\frac{1}{2}(y_{\\text{target}} - a)^2 = \\frac{1}{2}(1 - 1.3)^2 = 0.045\n",
    "     \\]\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - Compute the gradient of the loss with respect to the activation:\n",
    "     \\[\n",
    "     \\frac{\\partial L}{\\partial a} = -(y_{\\text{target}} - a) = -(1 - 1.3) = 0.3\n",
    "     \\]\n",
    "   - Compute the gradient of the activation with respect to the weighted sum:\n",
    "     \\[\n",
    "     \\frac{\\partial a}{\\partial z} = 1 \\quad \\text{(since it's a linear function)}\n",
    "     \\]\n",
    "   - Compute the gradient of the loss with respect to the weighted sum:\n",
    "     \\[\n",
    "     \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = 0.3 \\cdot 1 = 0.3\n",
    "     \\]\n",
    "   - Compute the gradient of the weighted sum with respect to the weight and bias:\n",
    "     \\[\n",
    "     \\frac{\\partial z}{\\partial w} = x = 2\n",
    "     \\]\n",
    "     \\[\n",
    "     \\frac{\\partial z}{\\partial b} = 1\n",
    "     \\]\n",
    "   - Compute the gradient of the loss with respect to the weight and bias using the chain rule:\n",
    "     \\[\n",
    "     \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = 0.3 \\cdot 2 = 0.6\n",
    "     \\]\n",
    "     \\[\n",
    "     \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = 0.3 \\cdot 1 = 0.3\n",
    "     \\]\n",
    "\n",
    "4. **Update Weights and Bias**:\n",
    "   Update the weight and bias using gradient descent:\n",
    "   \\[\n",
    "   w \\leftarrow w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} = 0.5 - 0.1 \\cdot 0.6 = 0.44\n",
    "   \\]\n",
    "   \\[\n",
    "   b \\leftarrow b - \\alpha \\cdot \\frac{\\partial L}{\\partial b} = 0.3 - 0.1 \\cdot 0.3 = 0.27\n",
    "   \\]\n",
    "\n",
    "5. **Repeat**:\n",
    "   Repeat steps 2-4 for a number of iterations or until convergence.\n",
    "\n",
    "This example demonstrates a single iteration of forward pass, backpropagation, and weight update for a single neuron with one weight and one bias. In practice, you would typically perform multiple iterations (epochs) of this process to train the network on a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A((Input Layer: x=2))\n",
    "    B((Neuron))\n",
    "    \n",
    "    A --> B\n",
    "    \n",
    "    subgraph Neuron\n",
    "        B\n",
    "        B --> C((Weighted Sum))\n",
    "        B --> D((Activation))\n",
    "        B --> E((dL/dz))\n",
    "    end\n",
    "    \n",
    "    C --> E\n",
    "    E --> D\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A((Input Layer: x=2))\n",
    "    B((Neuron))\n",
    "    \n",
    "    A --> B\n",
    "    \n",
    "    subgraph Neuron\n",
    "        B\n",
    "        B --> C((Weighted Sum))\n",
    "        B --> D((Activation))\n",
    "        B --> E((dL/dz))\n",
    "        B --> F((dz/dw))\n",
    "    end\n",
    "    \n",
    "    C --> F\n",
    "    E --> F\n",
    "    F --> D\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A((Input Layer: x=2))\n",
    "    B((Neuron))\n",
    "    \n",
    "    A --> B\n",
    "    \n",
    "    subgraph Neuron\n",
    "        B\n",
    "        B --> C((Weighted Sum))\n",
    "        B --> D((Activation))\n",
    "        B --> E((dL/dz))\n",
    "        B --> F((dz/dw))\n",
    "        B --> G((dz/db))\n",
    "    end\n",
    "    \n",
    "    C --> F\n",
    "    E --> F\n",
    "    F --> D\n",
    "    G --> D\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A((Input Layer: x=2))\n",
    "    B((Neuron))\n",
    "    \n",
    "    A --> B\n",
    "    \n",
    "    subgraph Neuron\n",
    "        B\n",
    "        B --> C((Weighted Sum))\n",
    "        B --> D((Activation))\n",
    "        B --> E((dL/dz))\n",
    "        B --> F((dz/dw))\n",
    "        B --> G((dz/db))\n",
    "        B --> H((dw))\n",
    "        B --> I((db))\n",
    "    end\n",
    "    \n",
    "    C --> F\n",
    "    E --> F\n",
    "    F --> D\n",
    "    G --> D\n",
    "    H --> J((Updated Weight))\n",
    "    I --> K((Updated Bias))\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
